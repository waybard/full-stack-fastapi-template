from fastapi import APIRouter, HTTPException
from typing import Optional

from .schemas import ChatRequest, ChatResponse, SummaryResponse
from .service import scrape_proceeding_data
from ..core.llm_service import LLMService
from ..core.session_manager import save_session, get_session, session_exists
from ..agents.config import AGENTS

router = APIRouter(prefix="/proceeding", tags=["proceedings"])

# Initialize LLM service
llm_service = LLMService()


@router.post("/{jurisdiction_id}/{proceeding_number}/chat/{agent_id}", response_model=ChatResponse)
async def chat_with_proceeding(
    jurisdiction_id: str,
    proceeding_number: str,
    agent_id: str,
    request: ChatRequest
) -> ChatResponse:
    """
    Chat with a proceeding using a specific agent in a single session.
    
    Args:
        jurisdiction_id: The jurisdiction identifier
        proceeding_number: The proceeding number
        agent_id: The agent to use for the chat
        request: The chat request containing the user's message
        
    Returns:
        ChatResponse: The response from the agent
    """
    # Validate agent exists
    if agent_id not in AGENTS:
        raise HTTPException(status_code=404, detail=f"Agent '{agent_id}' not found")
    
    # Create session ID
    session_id = f"{jurisdiction_id}_{proceeding_number}"
    
    # Check if session exists, if not create it
    if not session_exists(session_id):
        # Scrape proceeding data
        proceeding_text = scrape_proceeding_data(jurisdiction_id, proceeding_number)
        # Save to session
        save_session(session_id, proceeding_text)
    
    # Get proceeding text from session
    proceeding_text = get_session(session_id)
    if not proceeding_text:
        raise HTTPException(status_code=500, detail="Failed to retrieve proceeding data from session")
    
    # Get agent configuration
    agent_config = AGENTS[agent_id]
    system_prompt = agent_config["system_prompt"]
    llm_model = agent_config.get("llm_model")
    
    # Generate response using LLM service
    response_text = llm_service.generate_response(
        system_prompt=system_prompt,
        user_query=request.message,
        document_text=proceeding_text,
        llm_model=llm_model
    )
    
    return ChatResponse(response=response_text, agent_id=agent_id)


@router.post("/{jurisdiction_id}/{proceeding_number}/summary/{agent_id}", response_model=SummaryResponse)
async def summarize_proceeding(
    jurisdiction_id: str,
    proceeding_number: str,
    agent_id: str
) -> SummaryResponse:
    """
    Generate a summary of a proceeding using a specific agent.
    
    Args:
        jurisdiction_id: The jurisdiction identifier
        proceeding_number: The proceeding number
        agent_id: The agent to use for summarization
        
    Returns:
        SummaryResponse: The summary generated by the agent
    """
    # Validate agent exists
    if agent_id not in AGENTS:
        raise HTTPException(status_code=404, detail=f"Agent '{agent_id}' not found")
    
    # Validate agent is summary agent
    if agent_id != "summary_agent":
        raise HTTPException(status_code=400, detail="Only summary_agent can be used for summarization")
    
    # Scrape proceeding data (performed on each call)
    proceeding_text = scrape_proceeding_data(jurisdiction_id, proceeding_number)
    
    # Get agent configuration
    agent_config = AGENTS[agent_id]
    system_prompt = agent_config["system_prompt"]
    llm_model = agent_config.get("llm_model")
    
    # Generate summary using LLM service
    summary_text = llm_service.generate_response(
        system_prompt=system_prompt,
        user_query="Please provide a summary of this legal proceeding.",
        document_text=proceeding_text,
        llm_model=llm_model
    )
    
    return SummaryResponse(summary=summary_text, agent_id=agent_id)